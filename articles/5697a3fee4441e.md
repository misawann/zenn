---
title: "æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æœ‰åŠ¹æ¨¡å‹ã®æ§‹ç¯‰ã¨ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã¸ã®å¿œç”¨"
emoji: "ğŸ—¿"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["ml", "physics", "pytorch"]
published: false
---

ã“ã‚Œã¯[æ±äº¬å¤§å­¦ ç‰©å·¥/è¨ˆæ•° Advent Calendar 2022](https://adventar.org/calendars/7701)ã®ãŸã‚ã«æ›¸ã‹ã‚ŒãŸè¨˜äº‹ã§ã™ã€‚
æ©Ÿæ¢°å­¦ç¿’ã¨ç‰©ç†ã«ã¤ã„ã¦æ›¸ã„ã¦ã¿ã¾ã™ã€‚

# å°å…¥: Bogoliubov ä¸ç­‰å¼

ç‰©ç†ã§ã¯è§£æãŒé›£ã—ã„æ¨¡å‹ (Hamilitonian) ã®ä»£ã‚ã‚Šã«ã€ãã®ç‰©ç†çš„æ€§è³ªã‚’æ¥µåŠ›åæ˜ ã—ã¤ã¤ã‚‚è§£æãŒå®¹æ˜“ãªæ¨¡å‹ï¼ˆ**æœ‰åŠ¹æ¨¡å‹**ï¼‰ã‚’æ‰±ã„ãŸã„ã“ã¨ãŒã—ã°ã—ã°ã‚ã‚Šã¾ã™ã€‚
ä¾‹ãˆã°ã€ Ising æ¨¡å‹ã«ãŠã„ã¦æ³¨ç›®ã™ã‚‹ã‚¹ãƒ”ãƒ³ä»¥å¤–ã‹ã‚‰ã®å¯„ä¸ã‚’å¹³å‡åŒ–ã—ã€ã‚ã‚‹å ´ã‹ã‚‰ã®å½±éŸ¿ã¨è€ƒãˆã‚‹è¿‘ä¼¼ï¼ˆå¹³å‡å ´è¿‘ä¼¼ï¼‰ã®çµæœå¾—ã‚‰ã‚Œã‚‹æ¨¡å‹ã‚‚ãã®ä¸€ç¨®ã§ã™ã€‚

ãã“ã§ã€ã€Œæœ‰åŠ¹æ¨¡å‹ã‚’ã©ã®ã‚ˆã†ã«æ§‹ç¯‰ã™ã‚‹ã‹ã€ã¨ã„ã†ç–‘å•ãŒæµ®ã‹ã³ã¾ã™ã€‚ã¾ãŸã€ã¨ã‚Šã‚ã‘ Ising æ¨¡å‹ã§ã®å¹³å‡å ´è¿‘ä¼¼ãŒãªãœæ­£å½“åŒ–ã•ã‚Œã‚‹ã‹ã‚‚æ°—ã«ãªã‚Šã¾ã™ã€‚
Bogoliubov ä¸ç­‰å¼ã¯ãã‚Œã«å¯¾ã™ã‚‹ 1 ã¤ã®ç­”ãˆã‚’ä¸ãˆã¦ãã‚Œã¾ã™ã€‚[^1]
[^1]: Bogoliubov ä¸ç­‰å¼ã¯ 3A ã®ç‰©ç†å·¥å­¦æ¼”ç¿’ã§æ‰±ã„ã¾ã™ã€‚

$H$ ã‚’å³å¯†ãª Hamiltonianã€$H_0$ ã‚’æ‰±ã„ã‚„ã™ã„ Hamiltonian ã¨ã—ã¾ã™ã€‚
ã¾ãŸã€$\langle \dots \rangle_0$ã€$Z_0$ã§ãã‚Œãã‚Œ$H_0$ã«é–¢ã™ã‚‹æœŸå¾…å€¤ã€åˆ†é…é–¢æ•°ã‚’è¡¨ã™ã“ã¨ã«ã—ã¾ã™ã€‚ã“ã“ã§ã€

$$
F_v = -\frac{1}{\beta}\ln[\mathrm{Tr}(e^{-\beta(H_0 + \langle H-H_0\rangle _0)})] = -\frac{1}{\beta}\ln Z_0 + \langle	H-H_0\rangle_0
$$

ã§å®šç¾©ã•ã‚Œã‚‹å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼$F_v$ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®ä¸ç­‰å¼ãŒæˆç«‹ã—ã¾ã™ (Bogoliubov ä¸ç­‰å¼) ã€‚

$$
F \leq F_v
$$

ã“ã“ã§ã€$F$ã¯ç³»ã®å³å¯†ãªè‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã§ã€ $F = -\frac{1}{\beta}\ln \mathrm{Tr}[ e^{-\beta H}]$ ã§ã™ã€‚
ã¤ã¾ã‚Šã€**å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã§æœ‰åŠ¹æ¨¡å‹ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã™**ã€‚

$d$æ¬¡å…ƒ Ising æ¨¡å‹ã«ãŠã„ã¦ã€$H_0 = -\sum_ih_i\sigma_i$ã¨ã—ã€Bogoliubov ä¸ç­‰å¼ã‚’é©ç”¨ã™ã‚‹ã¨ã€è‡ªå·±ç„¡æ’ç€æ–¹ç¨‹å¼ $m=\tanh \beta zJm$ ãŒå¾—ã‚‰ã‚Œã€å¹³å‡å ´è¿‘ä¼¼ã®çµæœã¨ä¸€è‡´ã—ã¾ã™ã€‚
ï¼ˆå°‘ãªãã¨ã‚‚ Ising æ¨¡å‹ã«ãŠã‘ã‚‹ï¼‰å¹³å‡å ´è¿‘ä¼¼ã¯å¤‰åˆ†åŸç†ã®è¦³ç‚¹ã§æ­£å½“åŒ–ã•ã‚Œã‚‹ã‚ã‘ã§ã™ã­ã€‚
![ising](/images/self-learning-mc/ising_bogoliubov.jpg)
_$d$æ¬¡å…ƒ Ising æ¨¡å‹ã«ãŠã‘ã‚‹å¹³å‡å ´è¿‘ä¼¼ãŒå¤‰åˆ†æ³•ã®è¦³ç‚¹ã§æ­£å½“åŒ–ã•ã‚Œã‚‹ã“ã¨ã®è¨¼æ˜_

# æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æœ‰åŠ¹æ¨¡å‹ã®æ§‹ç¯‰

## ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³

Ising æ¨¡å‹ã®ä¾‹ã¯éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ã§ã€æ‰‹è¨ˆç®—ã§ç°¡å˜ã«æœ‰åŠ¹æ¨¡å‹ã‚’æ±‚ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚
ã—ã‹ã—ã€ä¸€èˆ¬ã®ã‚ˆã‚Šè¤‡é›‘ãªæ¨¡å‹ã§ã¯å›°é›£ã§ã‚ã‚‹ã“ã¨ãŒæƒ³å®šã•ã‚Œã¾ã™ã€‚ãã‚‚ãã‚‚æœ‰åŠ¹æ¨¡å‹ã®é©åˆ‡ãªå½¢ãŒæœªçŸ¥ã§ã€æ‰‹è¨ˆç®—ã§è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®è¨ˆç®—ãƒ»å¤‰åˆ†æ³•ã®é©ç”¨ã‚‚è‹¦ã§ã™ã€‚

ãã“ã§ã€è¨ˆç®—æ©Ÿã«ä¹—ã›ãŸããªã‚Šã¾ã™ã€‚
ã¨ã“ã‚ãŒã€è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’åˆ©ç”¨ã—ãŸå¤‰åˆ†æ³•ã¯è¨ˆç®—é‡ãŒå¤§ãã„ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚Ising æ¨¡å‹ã®ä¾‹ã§è¨€ã†ã¨ã€æœ€é©ãª$F_v$ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$h_i$ã‚’æ›´æ–°ã—ã¾ã™ãŒã€ãã®åº¦ã«$F_v$ã‚’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã«ã‚ˆã£ã¦æ¨å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã§ã™ã€‚

ä»Šå›ã¯ã€Liu et al.(2016) ã§ææ¡ˆã•ã‚ŒãŸæ©Ÿæ¢°å­¦ç¿’ã‚’ç”¨ã„ãŸæœ‰åŠ¹æ¨¡å‹ã®æ§‹ç¯‰æ‰‹æ³•ã«ã¤ã„ã¦ç´¹ä»‹ã—ã¾ã™ã€‚

## è¨ˆç®—ã‚¹ã‚­ãƒ¼ãƒ 

## å®Ÿè£…

ä»Šå›ã¯ã€2 æ¬¡å…ƒ Ising æ¨¡å‹ã«å¯¾ã—ã¦é©ç”¨ã—ã¦ã¿ã¾ã™ã€‚Liu et al. (2016) ã§å®Ÿé¨“ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«ã€ã‚ˆã‚Šè¤‡é›‘ãªæ¨¡å‹ã«å¯¾ã—ã¦ã‚‚é©ç”¨å¯èƒ½ã§ã™ãŒã€æ•¢ãˆã¦æœ€ã‚‚ç°¡å˜ãªä¾‹ã‚’æ‰±ã„ã¾ã™ã€‚

å®Ÿè£…ã«ã¯ Pytorch ã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚

ã¾ãšã€åŸå‹ã® Hamilotonian ã‚’å®šç¾©ã—ã¾ã™ã€‚
ã“ã“ã§ã€å‘¨æœŸå¢ƒç•Œæ¡ä»¶ã‚’èª²ã—ã¦ãŠã‚Šã€`S[(i + 1) % Lx, j]`ãªã©ã®ã‚ˆã†ã«ç°¡æ½”ã«æ›¸ã„ã¦ã„ã¾ã™ã€‚

```python
def ising2d(S: torch.Tensor, J: torch.Tensor) -> torch.Tensor:
    """original Hamiltonian of 2D Ising model

    Args:
        S (torch.Tensor): spin. (lattice size x, lattice size y)
        J (torch.Tensor): coefficient of spin interaction.

    Returns:
        torch.Tensor: original Hamiltonian
    """
    Lx, Ly = S.shape
    H = 0
    for i in range(Lx):
        for j in range(Ly):
            prod = neighbor_interact(S, i, j, Lx, Ly)
            H += prod
    H = J / 2 * H
    return H


def neighbor_interact(S: torch.Tensor, i: int, j: int, Lx: int, Ly: int) -> float:
    """calculate interaction with neighbors

    Args:
        S (torch.Tensor): spin. (lattice size x, lattice size y)
        i (int): index of position x.
        j (int): index of position y.
        Lx (int): lattice size of dimension x.
        Ly (int): lattice size of dimension y.

    Returns:
        float: interaction with neighbors.
    """
    return S[i, j] * (
        S[(i + 1) % Lx, j]
        + S[(i - 1) % Lx, j]
        + S[i, (j + 1) % Ly]
        + S[i, (j - 1) % Ly]
    )
```

æ¬¡ã«ã€å¹³å‡å ´ã® Hamiltonian ã‚’å®šç¾©ã—ã¾ã™ã€‚ã“ã“ã§ã€$h$ã¯ä»Šå›æœ€é©åŒ–ã—ãŸã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™ã€‚
ã¡ãªã¿ã«ã€[`torch.einsum`](https://pytorch.org/docs/stable/generated/torch.einsum.html)ã¯ã€Einstein ã®ç¸®ç´„è¨˜æ³•ã§ã€ç¬¬ä¸€å¼•æ•°ã«æ¸¡ã—ãŸå‹ã®æ¨ç§»ã«å¾“ã£ã¦è¨ˆç®—ã—ã¦ãã‚Œã‚‹ä¾¿åˆ©é–¢æ•°ã§ã™ã€‚

```python
def mean_field(S: torch.Tensor, h: torch.Tensor) -> torch.Tensor:
    """mean field Hamiltonian of 2D Ising model

    Args:
        S (torch.Tensor): spin. (batch size, lattice size x, lattice size y)
        h (torch.Tensor): coefficients for each spin. (batch size, lattice size x, lattice size y)

    Returns:
        torch.Tensor: mean field Hamiltonian
    """
    H = torch.einsum("bxy,bxy->b", S, h)
    return H
```

æœ€å¾Œã«ã€å­¦ç¿’ã®ãŸã‚ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
ã¾ãšã€äº¤æ›ç›¸äº’ä½œç”¨ã®ä¿‚æ•°$J$ã¨æ ¼å­ã®ã‚µã‚¤ã‚º$L_x$, $L_y$ã‚’å®šç¾©ã—ã¾ã™ã€‚

```python
class Ising2D:
    def __init__(self, J: float, Lx: int, Ly: int) -> None:
        """set basic constants of the system

        Args:
            J (float): coefficient of spin interaction.
            Lx (int): lattice size of dimension x.
            Ly (int): lattice size of dimension x.
        """
        self.J = J
        self.Lx = Lx
        self.Ly = Ly
```

æ¬¡ã«ã€`dataloader`ã‚’ä½œæˆã—ã¾ã™ã€‚ãƒãƒƒãƒå‡¦ç†ã‚’æ¥½ã«ã—ãŸã„ã ã‘ãªã®ã§ã€æœ¬è³ªã§ã¯ãªã„ã§ã™ã€‚

```python
def create_dataloader(self, n_samples: int, batch_size: int) -> DataLoader:
    """create dataloader given samples and batch size

    Args:
        n_samples (int): number of data to sample.
        batch_size (int): batch size

    Returns:
        DataLoader: dataloader for training or evaluation
    """
        X = 1 - 2 * (torch.rand(n_samples, self.Lx, self.Ly) < 0.5)
        X = X.float()
        Y = torch.FloatTensor([ising2d(x, self.J) for x in X])
        ds = TensorDataset(X, Y)
        dataloader = DataLoader(ds, batch_size=batch_size)
        return dataloader
```

ä»¥ä¸‹ãŒãƒ¡ã‚¤ãƒ³é–¢æ•°ç¾¤ã§ã™ã€‚
`train_loop`ã§ã¯`dataloader`ã‹ã‚‰å…¥å‡ºåŠ›ãƒšã‚¢ã‚’å–ã‚Šå‡ºã—ã€å¹³å‡å ´ã® Hamiltonian ã‚’è¨ˆç®—ã—ã¦ã€æå¤±ã‚’è¨ˆç®—ã—ã€èª¤å·®é€†ä¼æ’­ã§å‹¾é…ã‚’è¨ˆç®—ã—ã¦ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã¦ã„ã¾ã™ã€‚[^2]
[^2]: é…ã‚Œã°ã›ãªãŒã‚‰åŸ·ç­†æ™‚ã« github copilot ã‚’å–ã‚Šå…¥ã‚Œã¦ã¿ã¾ã—ãŸã€‚ã“ã®è‡ªç„¶æ–‡ã®ã»ã¨ã‚“ã©ã¯ copilot ã«ã‚ˆã‚‹ ææ¡ˆã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚å¹³å‡å ´ã® Hamiltonian ã¨ã„ã†å˜èªã¯æ•°åè¡Œå‰ã«ã‚ã‚‹ã®ã§ã€å¹…åºƒã„æ–‡è„ˆã‚’ç†è§£ã—ã¦ã„ãã†ã§ã™ã€‚docs ã‚’æ›¸ãã¨ãã«ä¾¿åˆ©ãã†ã§ã™ã­ã€‚æã‚‹ã¹ã—ã€‚

```python
def train_loop(
    self,
    dataloader: DataLoader,
    h: torch.Tensor,
    loss_fn: Callable,
    optimizer: Optimizer,
) -> float:
    """training loop

    Args:
        dataloader (DataLoader): dataloader
        h (torch.Tensor): coefficients for each spin. (batch size, lattice size x, lattice size y)
        loss_fn (Callable): loss function
        optimizer (Optimizer): optimizer

    Returns:
        float: final loss value
    """
    for X, Y in tqdm(dataloader, leave=False):
        H_eff = mean_field(X, h)
        loss = loss_fn(Y, H_eff)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    return loss
```

ä»¥ä¸Šã‚’ã¾ã¨ã‚ã¦ã€`__call__`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
å½“ç„¶ã®ã“ã¨ã§ã™ãŒã€å­¦ç¿’æ™‚ã®ãƒ‡ãƒ¼ã‚¿ (`train_dataloader`) ã¨ãƒ†ã‚¹ãƒˆæ™‚ã®ãƒ‡ãƒ¼ã‚¿ (`test_dataloader`)ã‚’åˆ†ã‘ã¦ã„ã¾ã™ã€‚
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿$h$ã¯ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã—ã€æŒ‡å®šã—ãŸã‚¨ãƒãƒƒã‚¯æ•°ãƒ»ãƒ‡ãƒ¼ã‚¿é‡ãƒ»ãƒãƒƒãƒã‚µã‚¤ã‚ºã§å­¦ç¿’ã‚’è¡Œã„ã¾ã™ã€‚
ã¾ãŸã€`optimizer`ã¯ã€[`SGD`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)ã¨[`Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)ã‚’é¸æŠã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚

```python
def __call__(
    self,
    lr: float = 0.001,
    train_samples: int = 100,
    test_samples: int = 100,
    epochs: int = 3,
    batch_size: int = 1,
    optimizer_name: str = "SGD",
    output_dir: str = None,
) -> None:
    """run optimization

    Args:
        lr (float, optional): learning rate. Defaults to 0.001.
        train_samples (int, optional): number of training samples. Defaults to 100.
        test_samples (int, optional): number of test samples. Defaults to 100.
        epochs (int, optional): number of training epochs. Defaults to 3.
        batch_size (int, optional): batch size for both training & evaluation. Defaults to 1.
        optimizer_name (str, optional): optimizer name. "SGD" & "Adam" can be used. Defaults to "SGD".
        output_dir (str, optional): output directory. should be specified when saving parameters. Defaults to None.
    """
    train_dataloader = self.create_dataloader(train_samples, batch_size)
    test_dataloader = self.create_dataloader(test_samples, batch_size)

    h = torch.rand((self.Lx, self.Ly), dtype=torch.float32)
    h = h.unsqueeze(0).repeat(batch_size, 1, 1)
    h.requires_grad_()
    loss_fn = MSELoss()

    if optimizer_name == "SGD":
        optimizer = torch.optim.SGD([h], lr=lr)
    elif optimizer_name == "Adam":
        optimizer = torch.optim.Adam([h], lr=lr)
    else:
        raise NotImplementedError

    with tqdm(range(epochs)) as pbar_epoch:
        for epoch in range(epochs):
            pbar_epoch.set_description("[Epoch %d]" % (epoch + 1))
            loss = self.train_loop(train_dataloader, h, loss_fn, optimizer)
            tqdm.write(f"loss at epoch{epoch+1}: {loss}")

    if not output_dir:
        return

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    torch.save(h, os.path.join(output_dir, "model.pth"))
```

ã¾ã¨ã‚ã‚‹ã¨ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
:::details å®Ÿè£…ã‚³ãƒ¼ãƒ‰

```python
class Ising2D:
    def __init__(self, J: float, Lx: int, Ly: int) -> None:
        """set basic constants of the system

        Args:
            J (float): coefficient of spin interaction.
            Lx (int): lattice size of dimension x.
            Ly (int): lattice size of dimension x.
        """
        self.J = J
        self.Lx = Lx
        self.Ly = Ly

    def create_dataloader(self, n_samples: int, batch_size: int) -> DataLoader:
        """create dataloader given samples and batch size

        Args:
            n_samples (int): number of data to sample.
            batch_size (int): batch size

        Returns:
            DataLoader: dataloader for training or evaluation
        """
        X = 1 - 2 * (torch.rand(n_samples, self.Lx, self.Ly) < 0.5)
        X = X.float()
        Y = torch.FloatTensor([ising2d(x, self.J) for x in X])
        ds = TensorDataset(X, Y)
        dataloader = DataLoader(ds, batch_size=batch_size)
        return dataloader

    def train_loop(
        self,
        dataloader: DataLoader,
        h: torch.Tensor,
        loss_fn: Callable,
        optimizer: Optimizer,
    ) -> float:
        """training loop

        Args:
            dataloader (DataLoader): dataloader
            h (torch.Tensor): coefficients for each spin. (batch size, lattice size x, lattice size y)
            loss_fn (Callable): loss function
            optimizer (Optimizer): optimizer

        Returns:
            float: final loss value
        """
        for X, Y in tqdm(dataloader, leave=False):
            H_eff = mean_field(X, h)
            loss = loss_fn(Y, H_eff)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        return loss

    def __call__(
        self,
        lr: float = 0.001,
        train_samples: int = 100,
        test_samples: int = 100,
        epochs: int = 3,
        batch_size: int = 1,
        optimizer_name: str = "SGD",
        output_dir: str = None,
    ) -> None:
        """run optimization

        Args:
            lr (float, optional): learning rate. Defaults to 0.001.
            train_samples (int, optional): number of training samples. Defaults to 100.
            test_samples (int, optional): number of test samples. Defaults to 100.
            epochs (int, optional): number of training epochs. Defaults to 3.
            batch_size (int, optional): batch size for both training & evaluation. Defaults to 1.
            optimizer_name (str, optional): optimizer name. "SGD" & "Adam" can be used. Defaults to "SGD".
            output_dir (str, optional): output directory. should be specified when saving parameters. Defaults to None.
        """
        train_dataloader = self.create_dataloader(train_samples, batch_size)
        test_dataloader = self.create_dataloader(test_samples, batch_size)

        h = torch.rand((self.Lx, self.Ly), dtype=torch.float32)
        h = h.unsqueeze(0).repeat(batch_size, 1, 1)
        h.requires_grad_()
        loss_fn = MSELoss()

        if optimizer_name == "SGD":
            optimizer = torch.optim.SGD([h], lr=lr)
        elif optimizer_name == "Adam":
            optimizer = torch.optim.Adam([h], lr=lr)
        else:
            raise NotImplementedError

        with tqdm(range(epochs)) as pbar_epoch:
            for epoch in range(epochs):
                pbar_epoch.set_description("[Epoch %d]" % (epoch + 1))
                loss = self.train_loop(train_dataloader, h, loss_fn, optimizer)
                tqdm.write(f"loss at epoch{epoch+1}: {loss}")

        if not output_dir:
            return

        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        torch.save(h, os.path.join(output_dir, "model.pth"))
```

:::

## å®Ÿé¨“çµæœ

# è‡ªå·±å­¦ç¿’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³• (self-learning Monte-Carlo)

## ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³

## å®Ÿè£…

# ã•ã„ã”ã«

æ©Ÿæ¢°å­¦ç¿’ Ã— ç‰©ç†ã®ä¸€ä¾‹ã¨ã—ã¦ã€è‡ªå·±å­¦ç¿’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã‚’æ‰±ã£ã¦ã¿ã¾ã—ãŸã€‚
å®Ÿè£…ã‚„ç™ºæƒ³ã¯ã‚·ãƒ³ãƒ—ãƒ«ãªãŒã‚‰é©ç”¨ç¯„å›²ã®åºƒã„æ‰‹æ³•ã§ã€å®Ÿé¨“ã—ç”²æ–ãŒã‚ã‚Šãã†ã§ã™ã€‚
ã‚ˆã‚Šè¤‡é›‘ãª Hamiltonian ã«é©ç”¨ã™ã‚‹è©±ã¯æ°—ãŒå‘ã„ãŸã‚‰æ›¸ã„ã¦ã¿ã¾ã™ã€‚

ã¾ãŸã€ä»Šå›ã¯ã€æœ‰åŠ¹æ¨¡å‹ã®å½¢ã‚’ã‚ã‚‹ç¨‹åº¦ä»®å®šã™ã‚‹ã“ã¨ã§ã€ãã®çµåˆå®šæ•°ã‚’äºˆæ¸¬ã™ã‚‹ã¨ã„ã†ç°¡å˜ãªã‚¿ã‚¹ã‚¯ã«è½ã¨ã—è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã—ãŸã€‚ã—ã‹ã—ã€ç†æƒ³çš„ã«ã¯ãã†ã„ã†ä»®å®šã‚‚å¤–ã—ãŸã„ã¨ã“ã‚ã§ã™ã€‚
ãã“ã§ã€æ·±å±¤å­¦ç¿’ã‚’åˆ©ç”¨ã—ãŸæ‰‹æ³•ãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®æ‰‹æ³•ã§ã¯ã€

è‡ªå·±å­¦ç¿’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã‚’é‡å­ç³»ã«é©ç”¨ã™ã‚‹è©±ã‚‚ã‚ã£ãŸã‚Šã¨ã€å¾Œç¶šã®ç ”ç©¶ã‚‚ç››ã‚“ã«æ„Ÿã˜ã¾ã™ã€‚ã“ã‚Œã‚‚æ°—ãŒå‘ã„ãŸã‚‰ã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—ã—ã¦ã¾ãŸæ›¸ã„ã¦ã¿ãŸã„ã¨æ€ã„ã¾ã™ã€‚

# å‚è€ƒæ–‡çŒ®

# ä½™è«‡

ã“ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’æ‰±ã£ãŸã®ã«æ·±ã„ç†ç”±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚3A ã®ç‰©ç†å·¥å­¦å®Ÿé¨“ã§é‡å­ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã«æŒ«æŠ˜ã—ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•å‘¨è¾ºã§æ¢ã£ã¦ã„ãŸã¨ã“ã‚å¶ç„¶è¦‹ã¤ã‘ãŸã®ãŒãã£ã‹ã‘ã§ã™ã€‚

\
ã‚¢ãƒ‰ã‚«ãƒ¬ã®ä»–ã®å€™è£œã¨ã—ã¦ã¯ä»¥ä¸‹ã‚’è€ƒãˆã¦ã„ã¾ã—ãŸã€‚

- ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã®å†å¸°æ€§ã«é–¢ã™ã‚‹ Polya ã®å®šç†
  - 3S ã®ç¢ºç‡æ•°ç†å·¥å­¦ã‚„ 3A ã®å¿œç”¨çµ±è¨ˆã§å†…å®¹ã ã‘æ‰±ã‚ã‚ŒãŸå®šç†
  - æ§˜ã€…ãªè¨¼æ˜ãŒã‚ã‚‹ã‚‰ã—ã„
- æ·±å±¤å­¦ç¿’ã§ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
  - æ¥­å‹™ã§ã‚„ã£ã¦ã„ã¦ååˆ†ã‚¤ãƒ³ãƒ—ãƒƒãƒˆãƒ»ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆãŒã‚ã‚‹ã®ã§ã‚„ã‚ã¾ã—ãŸ
- ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
  - è¶£å‘³ã®è©±
  - è‡ªå·±æº€ã«ãªã£ã¦ã—ã¾ã†ã®ã§æ–­å¿µ

\
Zenn ã§åŸ·ç­†ã™ã‚‹ã®ã¯åˆã‚ã¦ã§ã—ãŸãŒã€github é€£æºã¨ VSCode æ‹¡å¼µ (Zenn Editor) ãŒä¾¿åˆ©ã§ã—ãŸã€‚ä»¥ä¸‹ã®è¨˜äº‹ã‚’å‚è€ƒã«ã—ã¾ã—ãŸã€‚

- [GitHub ãƒªãƒã‚¸ãƒˆãƒªã§ Zenn ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç®¡ç†ã™ã‚‹](https://zenn.dev/zenn/articles/connect-to-github)
- [Zenn ã®åŸ·ç­†ã‚’æ”¯æ´ã™ã‚‹ VSCode æ‹¡å¼µ Zenn Editor](https://zenn.dev/negokaz/articles/aa4e12b76d516597a00e)
  ![zenn-editor](/images/self-learning-mc/zenn_editor.jpeg)
