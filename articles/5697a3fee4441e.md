---
title: "機械学習による有効模型の構築とモンテカルロ法への応用"
emoji: "🗿"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["ml", "physics", "pytorch"]
published: false
---

これは[東京大学 物工/計数 Advent Calendar 2022](https://adventar.org/calendars/7701)のために書かれた記事です。
機械学習と物理について書いてみます。

# 導入: Bogoliubov 不等式

物理では解析が難しい模型 (Hamilitonian) の代わりに、その物理的性質を極力反映しつつも解析が容易な模型（**有効模型**）を扱いたいことがしばしばあります。
例えば、 Ising 模型において注目するスピン以外からの寄与を平均化し、ある場からの影響と考える近似（平均場近似）の結果得られる模型もその一種です。

そこで、「有効模型をどのように構築するか」という疑問が浮かびます。また、とりわけ Ising 模型での平均場近似がなぜ正当化されるかも気になります。
Bogoliubov 不等式はそれに対する 1 つの答えを与えてくれます。[^1]
[^1]: Bogoliubov 不等式は 3A の物理工学演習で扱います。

$H$ を厳密な Hamiltonian、$H_0$ を扱いやすい Hamiltonian とします。
また、$\langle \dots \rangle_0$、$Z_0$でそれぞれ$H_0$に関する期待値、分配関数を表すことにします。ここで、

$$
F_v = -\frac{1}{\beta}\ln[\mathrm{Tr}(e^{-\beta(H_0 + \langle H-H_0\rangle _0)})] = -\frac{1}{\beta}\ln Z_0 + \langle	H-H_0\rangle_0
$$

で定義される変分自由エネルギー$F_v$に対して、以下の不等式が成立します (Bogoliubov 不等式) 。

$$
F \leq F_v
$$

ここで、$F$は系の厳密な自由エネルギーで、 $F = -\frac{1}{\beta}\ln \mathrm{Tr}[ e^{-\beta H}]$ です。
つまり、**変分自由エネルギーを最小化することで有効模型を得ることができます**。

$d$次元 Ising 模型において、$H_0 = -\sum_ih_i\sigma_i$とし、Bogoliubov 不等式を適用すると、自己無撞着方程式 $m=\tanh \beta zJm$ が得られ、平均場近似の結果と一致します。
（少なくとも Ising 模型における）平均場近似は変分原理の観点で正当化されるわけですね。
![ising](/images/self-learning-mc/ising_bogoliubov.jpg)
_$d$次元 Ising 模型における平均場近似が変分法の観点で正当化されることの証明_

# 機械学習による有効模型の構築

## モチベーション

Ising 模型の例は非常にシンプルで、手計算で簡単に有効模型を求めることができました。
しかし、一般のより複雑な模型では困難であることが想定されます。そもそも有効模型の適切な形が未知で、手計算で自由エネルギーの計算・変分法の適用も苦です。

そこで、計算機に乗せたくなります。
ところが、自由エネルギーを利用した変分法は計算量が大きいと考えられます。Ising 模型の例で言うと、最適な$F_v$を見つけるためにパラメータ$h_i$を更新しますが、その度に$F_v$をモンテカルロ法によって推定する必要があるためです。

今回は、Liu et al.(2016) で提案された機械学習を用いた有効模型の構築手法について紹介します。

## 計算スキーム

## 実装

今回は、2 次元 Ising 模型に対して適用してみます。Liu et al. (2016) で実験されているように、より複雑な模型に対しても適用可能ですが、敢えて最も簡単な例を扱います。

実装には Pytorch を使用しました。

まず、原型の Hamilotonian を定義します。
ここで、周期境界条件を課しており、`S[(i + 1) % Lx, j]`などのように簡潔に書いています。

```python
def ising2d(S: torch.Tensor, J: torch.Tensor) -> torch.Tensor:
    """original Hamiltonian of 2D Ising model

    Args:
        S (torch.Tensor): spin. (lattice size x, lattice size y)
        J (torch.Tensor): coefficient of spin interaction.

    Returns:
        torch.Tensor: original Hamiltonian
    """
    Lx, Ly = S.shape
    H = 0
    for i in range(Lx):
        for j in range(Ly):
            prod = neighbor_interact(S, i, j, Lx, Ly)
            H += prod
    H = J / 2 * H
    return H


def neighbor_interact(S: torch.Tensor, i: int, j: int, Lx: int, Ly: int) -> float:
    """calculate interaction with neighbors

    Args:
        S (torch.Tensor): spin. (lattice size x, lattice size y)
        i (int): index of position x.
        j (int): index of position y.
        Lx (int): lattice size of dimension x.
        Ly (int): lattice size of dimension y.

    Returns:
        float: interaction with neighbors.
    """
    return S[i, j] * (
        S[(i + 1) % Lx, j]
        + S[(i - 1) % Lx, j]
        + S[i, (j + 1) % Ly]
        + S[i, (j - 1) % Ly]
    )
```

次に、平均場の Hamiltonian を定義します。ここで、$h$は今回最適化したいパラメータです。
ちなみに、[`torch.einsum`](https://pytorch.org/docs/stable/generated/torch.einsum.html)は、Einstein の縮約記法で、第一引数に渡した型の推移に従って計算してくれる便利関数です。

```python
def mean_field(S: torch.Tensor, h: torch.Tensor) -> torch.Tensor:
    """mean field Hamiltonian of 2D Ising model

    Args:
        S (torch.Tensor): spin. (batch size, lattice size x, lattice size y)
        h (torch.Tensor): coefficients for each spin. (batch size, lattice size x, lattice size y)

    Returns:
        torch.Tensor: mean field Hamiltonian
    """
    H = torch.einsum("bxy,bxy->b", S, h)
    return H
```

最後に、学習のためのメインクラスを実装します。
まず、交換相互作用の係数$J$と格子のサイズ$L_x$, $L_y$を定義します。

```python
class Ising2D:
    def __init__(self, J: float, Lx: int, Ly: int) -> None:
        """set basic constants of the system

        Args:
            J (float): coefficient of spin interaction.
            Lx (int): lattice size of dimension x.
            Ly (int): lattice size of dimension x.
        """
        self.J = J
        self.Lx = Lx
        self.Ly = Ly
```

次に、`dataloader`を作成します。バッチ処理を楽にしたいだけなので、本質ではないです。

```python
def create_dataloader(self, n_samples: int, batch_size: int) -> DataLoader:
    """create dataloader given samples and batch size

    Args:
        n_samples (int): number of data to sample.
        batch_size (int): batch size

    Returns:
        DataLoader: dataloader for training or evaluation
    """
        X = 1 - 2 * (torch.rand(n_samples, self.Lx, self.Ly) < 0.5)
        X = X.float()
        Y = torch.FloatTensor([ising2d(x, self.J) for x in X])
        ds = TensorDataset(X, Y)
        dataloader = DataLoader(ds, batch_size=batch_size)
        return dataloader
```

以下がメイン関数群です。
`train_loop`では`dataloader`から入出力ペアを取り出し、平均場の Hamiltonian を計算して、損失を計算し、誤差逆伝播で勾配を計算して、パラメータを更新しています。[^2]
[^2]: 遅ればせながら執筆時に github copilot を取り入れてみました。この自然文のほとんどは copilot による 提案を採用しています。平均場の Hamiltonian という単語は数十行前にあるので、幅広い文脈を理解していそうです。docs を書くときに便利そうですね。恐るべし。

```python
def train_loop(
    self,
    dataloader: DataLoader,
    h: torch.Tensor,
    loss_fn: Callable,
    optimizer: Optimizer,
) -> float:
    """training loop

    Args:
        dataloader (DataLoader): dataloader
        h (torch.Tensor): coefficients for each spin. (batch size, lattice size x, lattice size y)
        loss_fn (Callable): loss function
        optimizer (Optimizer): optimizer

    Returns:
        float: final loss value
    """
    for X, Y in tqdm(dataloader, leave=False):
        H_eff = mean_field(X, h)
        loss = loss_fn(Y, H_eff)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    return loss
```

以上をまとめて、`__call__`メソッドを実装します。
当然のことですが、学習時のデータ (`train_dataloader`) とテスト時のデータ (`test_dataloader`)を分けています。
パラメータ$h$はランダムに初期化し、指定したエポック数・データ量・バッチサイズで学習を行います。
また、`optimizer`は、[`SGD`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)と[`Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)を選択できるようにしています。

```python
def __call__(
    self,
    lr: float = 0.001,
    train_samples: int = 100,
    test_samples: int = 100,
    epochs: int = 3,
    batch_size: int = 1,
    optimizer_name: str = "SGD",
    output_dir: str = None,
) -> None:
    """run optimization

    Args:
        lr (float, optional): learning rate. Defaults to 0.001.
        train_samples (int, optional): number of training samples. Defaults to 100.
        test_samples (int, optional): number of test samples. Defaults to 100.
        epochs (int, optional): number of training epochs. Defaults to 3.
        batch_size (int, optional): batch size for both training & evaluation. Defaults to 1.
        optimizer_name (str, optional): optimizer name. "SGD" & "Adam" can be used. Defaults to "SGD".
        output_dir (str, optional): output directory. should be specified when saving parameters. Defaults to None.
    """
    train_dataloader = self.create_dataloader(train_samples, batch_size)
    test_dataloader = self.create_dataloader(test_samples, batch_size)

    h = torch.rand((self.Lx, self.Ly), dtype=torch.float32)
    h = h.unsqueeze(0).repeat(batch_size, 1, 1)
    h.requires_grad_()
    loss_fn = MSELoss()

    if optimizer_name == "SGD":
        optimizer = torch.optim.SGD([h], lr=lr)
    elif optimizer_name == "Adam":
        optimizer = torch.optim.Adam([h], lr=lr)
    else:
        raise NotImplementedError

    with tqdm(range(epochs)) as pbar_epoch:
        for epoch in range(epochs):
            pbar_epoch.set_description("[Epoch %d]" % (epoch + 1))
            loss = self.train_loop(train_dataloader, h, loss_fn, optimizer)
            tqdm.write(f"loss at epoch{epoch+1}: {loss}")

    if not output_dir:
        return

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    torch.save(h, os.path.join(output_dir, "model.pth"))
```

まとめると以下のようになります。
:::details 実装コード

```python
class Ising2D:
    def __init__(self, J: float, Lx: int, Ly: int) -> None:
        """set basic constants of the system

        Args:
            J (float): coefficient of spin interaction.
            Lx (int): lattice size of dimension x.
            Ly (int): lattice size of dimension x.
        """
        self.J = J
        self.Lx = Lx
        self.Ly = Ly

    def create_dataloader(self, n_samples: int, batch_size: int) -> DataLoader:
        """create dataloader given samples and batch size

        Args:
            n_samples (int): number of data to sample.
            batch_size (int): batch size

        Returns:
            DataLoader: dataloader for training or evaluation
        """
        X = 1 - 2 * (torch.rand(n_samples, self.Lx, self.Ly) < 0.5)
        X = X.float()
        Y = torch.FloatTensor([ising2d(x, self.J) for x in X])
        ds = TensorDataset(X, Y)
        dataloader = DataLoader(ds, batch_size=batch_size)
        return dataloader

    def train_loop(
        self,
        dataloader: DataLoader,
        h: torch.Tensor,
        loss_fn: Callable,
        optimizer: Optimizer,
    ) -> float:
        """training loop

        Args:
            dataloader (DataLoader): dataloader
            h (torch.Tensor): coefficients for each spin. (batch size, lattice size x, lattice size y)
            loss_fn (Callable): loss function
            optimizer (Optimizer): optimizer

        Returns:
            float: final loss value
        """
        for X, Y in tqdm(dataloader, leave=False):
            H_eff = mean_field(X, h)
            loss = loss_fn(Y, H_eff)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        return loss

    def __call__(
        self,
        lr: float = 0.001,
        train_samples: int = 100,
        test_samples: int = 100,
        epochs: int = 3,
        batch_size: int = 1,
        optimizer_name: str = "SGD",
        output_dir: str = None,
    ) -> None:
        """run optimization

        Args:
            lr (float, optional): learning rate. Defaults to 0.001.
            train_samples (int, optional): number of training samples. Defaults to 100.
            test_samples (int, optional): number of test samples. Defaults to 100.
            epochs (int, optional): number of training epochs. Defaults to 3.
            batch_size (int, optional): batch size for both training & evaluation. Defaults to 1.
            optimizer_name (str, optional): optimizer name. "SGD" & "Adam" can be used. Defaults to "SGD".
            output_dir (str, optional): output directory. should be specified when saving parameters. Defaults to None.
        """
        train_dataloader = self.create_dataloader(train_samples, batch_size)
        test_dataloader = self.create_dataloader(test_samples, batch_size)

        h = torch.rand((self.Lx, self.Ly), dtype=torch.float32)
        h = h.unsqueeze(0).repeat(batch_size, 1, 1)
        h.requires_grad_()
        loss_fn = MSELoss()

        if optimizer_name == "SGD":
            optimizer = torch.optim.SGD([h], lr=lr)
        elif optimizer_name == "Adam":
            optimizer = torch.optim.Adam([h], lr=lr)
        else:
            raise NotImplementedError

        with tqdm(range(epochs)) as pbar_epoch:
            for epoch in range(epochs):
                pbar_epoch.set_description("[Epoch %d]" % (epoch + 1))
                loss = self.train_loop(train_dataloader, h, loss_fn, optimizer)
                tqdm.write(f"loss at epoch{epoch+1}: {loss}")

        if not output_dir:
            return

        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        torch.save(h, os.path.join(output_dir, "model.pth"))
```

:::

## 実験結果

# 自己学習モンテカルロ法 (self-learning Monte-Carlo)

## モチベーション

## 実装

# さいごに

機械学習 × 物理の一例として、自己学習モンテカルロ法を扱ってみました。
実装や発想はシンプルながら適用範囲の広い手法で、実験し甲斐がありそうです。
より複雑な Hamiltonian に適用する話は気が向いたら書いてみます。

また、今回は、有効模型の形をある程度仮定することで、その結合定数を予測するという簡単なタスクに落とし込むことができました。しかし、理想的にはそういう仮定も外したいところです。
そこで、深層学習を利用した手法が提案されています。この手法では、

自己学習モンテカルロ法を量子系に適用する話もあったりと、後続の研究も盛んに感じます。これも気が向いたらキャッチアップしてまた書いてみたいと思います。

# 参考文献

# 余談

このトピックを扱ったのに深い理由はありません。3A の物理工学実験で量子モンテカルロ法に挫折しモンテカルロ法周辺で探っていたところ偶然見つけたのがきっかけです。

\
アドカレの他の候補としては以下を考えていました。

- ランダムウォークの再帰性に関する Polya の定理
  - 3S の確率数理工学や 3A の応用統計で内容だけ扱われた定理
  - 様々な証明があるらしい
- 深層学習で物理シミュレーション
  - 業務でやっていて十分インプット・アウトプットがあるのでやめました
- レコードコレクション
  - 趣味の話
  - 自己満になってしまうので断念

\
Zenn で執筆するのは初めてでしたが、github 連携と VSCode 拡張 (Zenn Editor) が便利でした。以下の記事を参考にしました。

- [GitHub リポジトリで Zenn のコンテンツを管理する](https://zenn.dev/zenn/articles/connect-to-github)
- [Zenn の執筆を支援する VSCode 拡張 Zenn Editor](https://zenn.dev/negokaz/articles/aa4e12b76d516597a00e)
  ![zenn-editor](/images/self-learning-mc/zenn_editor.jpeg)
