---
title: "æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æœ‰åŠ¹æ¨¡åž‹ã®æ§‹ç¯‰ã¨ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã¸ã®å¿œç”¨"
emoji: "ðŸ—¿"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["ml", "physics", "pytorch"]
published: false
---

ã“ã‚Œã¯[æ±äº¬å¤§å­¦ ç‰©å·¥/è¨ˆæ•° Advent Calendar 2022](https://adventar.org/calendars/7701)ã®ãŸã‚ã«æ›¸ã‹ã‚ŒãŸè¨˜äº‹ã§ã™ã€‚

æ©Ÿæ¢°å­¦ç¿’ã¨ç‰©ç†ã®ãƒˆãƒ”ãƒƒã‚¯ã¨ã—ã¦ã€æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æœ‰åŠ¹æ¨¡åž‹ã®æ§‹ç¯‰ã¨ãã®ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã¸ã®å¿œç”¨ã‚’ç´¹ä»‹ã—ã‚ˆã†ã¨æ€ã„ã¾ã™ã€‚å‰è€…ã«ã¤ã„ã¦ã¯ã€å®Ÿè£…ã«ã‚‚è§¦ã‚Œã¾ã™ã€‚

https://github.com/misawann/self-learningMC

# å°Žå…¥: Gibbs-Bogoliubov-Feynman ã®ä¸ç­‰å¼

ç‰©ç†ã§ã¯ã€è§£æžãŒé›£ã—ã„æ¨¡åž‹ (Hamiltonian) ã®ä»£ã‚ã‚Šã«ã€è‡ªç”±åº¦ãŒå°ã•ãã€ã‚ã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®ç¯„å›²ã§ã¯ä¸€è‡´ã™ã‚‹æ¨¡åž‹ï¼ˆ**æœ‰åŠ¹æ¨¡åž‹**ï¼‰ã‚’æ‰±ã„ãŸã„ã“ã¨ãŒã—ã°ã—ã°ã‚ã‚Šã¾ã™ã€‚

ä¾‹ãˆã°ã€ Ising æ¨¡åž‹ã«ãŠã„ã¦æ³¨ç›®ã™ã‚‹ã‚¹ãƒ”ãƒ³ä»¥å¤–ã‹ã‚‰ã®å¯„ä¸Žã‚’å¹³å‡åŒ–ã—ã€æœ‰åŠ¹ç£å ´ã‹ã‚‰ã®å½±éŸ¿ã¨è€ƒãˆã€ä¸€ä½“å•é¡Œã¨ã—ã¦æ‰±ã†è¿‘ä¼¼ï¼ˆå¹³å‡å ´è¿‘ä¼¼ï¼‰ã®çµæžœå¾—ã‚‰ã‚Œã‚‹æ¨¡åž‹ã‚‚ãã®ä¸€ç¨®ã§ã™ã€‚

ãã“ã§ã€ã€Œæœ‰åŠ¹æ¨¡åž‹ã‚’ã©ã®ã‚ˆã†ã«æ§‹ç¯‰ã™ã‚‹ã‹ã€ã¨ã„ã†ç–‘å•ãŒæµ®ã‹ã³ã¾ã™ã€‚ã¾ãŸã€å¹³å‡å ´è¿‘ä¼¼ãŒãªãœæ­£å½“åŒ–ã•ã‚Œã‚‹ã‹ã‚‚æ°—ã«ãªã‚Šã¾ã™ã€‚
Gibbs-Bogoliubov-Feynman (GBF) ã®ä¸ç­‰å¼ã¯ãã‚Œã«å¯¾ã™ã‚‹ 1 ã¤ã®ç­”ãˆã‚’ä¸Žãˆã¦ãã‚Œã¾ã™ã€‚[^1]
[^1]: GBF ä¸ç­‰å¼ã¯ 3A ã®ç‰©ç†å·¥å­¦æ¼”ç¿’ã§æ‰±ã„ã¾ã™ã€‚

$H$ ã‚’åŽ³å¯†ãª Hamiltonianã€$H_0$ ã‚’è‡ªç”±åº¦ã®ä½Žã„ Hamiltonian ã¨ã—ã¾ã™ã€‚
ã¾ãŸã€$\langle \dots \rangle_0$ã€$Z_0$ ã§ãã‚Œãžã‚Œ $H_0$ ã«é–¢ã™ã‚‹æœŸå¾…å€¤ã€åˆ†é…é–¢æ•°ã‚’è¡¨ã™ã“ã¨ã«ã—ã¾ã™ã€‚ã“ã“ã§ã€

$$
F_v = -\frac{1}{\beta}\ln[\mathrm{Tr}(e^{-\beta(H_0 + \langle H-H_0\rangle _0)})] = -\frac{1}{\beta}\ln Z_0 + \langle	H-H_0\rangle_0
$$

ã§å®šç¾©ã•ã‚Œã‚‹å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼$F_v$ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®ä¸ç­‰å¼ãŒæˆç«‹ã—ã¾ã™ã€‚

$$
F_v - F = k_B T D_{\mathrm{KL}}(\rho_0 || \rho) \geq 0
$$

ã“ã“ã§ã€$F$ ã¯ç³»ã®åŽ³å¯†ãªè‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã§ã€ $F = -\frac{1}{\beta}\ln \mathrm{Tr}[ e^{-\beta H}]$ ã§ã™ã€‚ã¾ãŸã€$D_{\mathrm{KL}}$ã¯ KL divergence ã§ã€$\rho_0 = e^{-\beta H_0}/Z_0,\ \rho = e^{-\beta H}/Z$ ã§ã™ã€‚
ã¤ã¾ã‚Šã€**å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã€ã‚ã‚‹ã„ã¯ KL divergence ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã§æœ‰åŠ¹æ¨¡åž‹ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã™**ã€‚

$d$ æ¬¡å…ƒ Ising æ¨¡åž‹ã«ãŠã„ã¦ã€$\displaystyle{H_0 = -\sum_ih_i\sigma_i}$ ã¨ã—ã¦ GBF ä¸ç­‰å¼ã‚’é©ç”¨ã™ã‚‹ã¨ã€è‡ªå·±ç„¡æ’žç€æ–¹ç¨‹å¼ $m=\tanh \beta zJm$ ãŒå¾—ã‚‰ã‚Œã€å¹³å‡å ´è¿‘ä¼¼ã®çµæžœã¨ä¸€è‡´ã—ã¾ã™ã€‚
ï¼ˆå°‘ãªãã¨ã‚‚ Ising æ¨¡åž‹ã«ãŠã‘ã‚‹ï¼‰å¹³å‡å ´è¿‘ä¼¼ã¯å¤‰åˆ†åŽŸç†ã®è¦³ç‚¹ã§æ­£å½“åŒ–ã•ã‚Œã‚‹ã‚ã‘ã§ã™ã­ã€‚
![ising](https://storage.googleapis.com/zenn-user-upload/f234c2f0f4d5-20221210.jpg)
_$d$æ¬¡å…ƒ Ising æ¨¡åž‹ã«ãŠã‘ã‚‹å¹³å‡å ´è¿‘ä¼¼ãŒå¤‰åˆ†æ³•ã®è¦³ç‚¹ã§æ­£å½“åŒ–ã•ã‚Œã‚‹ã“ã¨ã®ç•¥è¨¼_

# æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æœ‰åŠ¹æ¨¡åž‹ã®æ§‹ç¯‰

Ising æ¨¡åž‹ã®ä¾‹ã¯éžå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ã§ã€æ‰‹è¨ˆç®—ã§ç°¡å˜ã«æœ‰åŠ¹æ¨¡åž‹ã‚’æ±‚ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚
ã—ã‹ã—ã€ä¸€èˆ¬ã®ã‚ˆã‚Šè¤‡é›‘ãªæ¨¡åž‹ã§ã¯å›°é›£ã§ã‚ã‚‹ã“ã¨ãŒæƒ³å®šã•ã‚Œã¾ã™ã€‚ãã‚‚ãã‚‚æœ‰åŠ¹æ¨¡åž‹ã®é©åˆ‡ãªå½¢ãŒæœªçŸ¥ã§ã€æ‰‹è¨ˆç®—ã§è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®è¨ˆç®—ãƒ»å¤‰åˆ†æ³•ã®é©ç”¨ã‚‚è‹¦ã§ã™ã€‚

ãã“ã§ã€è¨ˆç®—æ©Ÿã«ä¹—ã›ãŸããªã‚Šã¾ã™ã€‚
ã¨ã“ã‚ãŒã€è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’åˆ©ç”¨ã—ãŸå¤‰åˆ†æ³•ã¯è¨ˆç®—é‡ãŒå¤§ãã„ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚Ising æ¨¡åž‹ã®ä¾‹ã§è¨€ã†ã¨ã€æœ€é©ãª $F_v$ ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $h_i$ ã‚’æ›´æ–°ã—ã¾ã™ãŒã€ãã®åº¦ã« $F_v$ ã¾ãŸã¯ $\rho_0$ ã‚’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã«ã‚ˆã£ã¦æŽ¨å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã§ã™ã€‚

ä»Šå›žã¯ã€Liu et al. (2016) ã‚„ Fujita et al. (2017) ã‚’å‚è€ƒã«ã—ã¦ã€æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æœ‰åŠ¹æ¨¡åž‹ã®æ§‹ç¯‰æ‰‹æ³•ã«ã¤ã„ã¦ç´¹ä»‹ã—ã¾ã™ã€‚

## è¨ˆç®—ã‚¹ã‚­ãƒ¼ãƒ 

åŽ³å¯†ãª Hamiltonian ã‚’ $H$ã€è©¦è¡Œ Hamiltonian ã‚’ $H_{\mathrm {trial}}$ ã¨ã—ã¾ã™ã€‚$H_{\mathrm {trial}}$ ã¯äº¤æ›ç›¸äº’ä½œç”¨ã‚„å¤–å ´ã¨ã®ç›¸äº’ä½œç”¨ã¨ã„ã£ãŸé … $H_i$ ã®ç·šå½¢å’Œã§æ›¸ã‘ã‚‹ã¨ä»®å®šã—ã€

$$
H_{\mathrm {trial}} = \sum_i c_i H_i
$$

ã¨ã—ã¾ã™ã€‚ã“ã“ã§ã€$c_i$ ã¯å­¦ç¿’ã«ã‚ˆã£ã¦æ±ºã‚ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™ã€‚ã¤ã¾ã‚Šã€æœ‰åŠ¹ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã‚’æ±‚ã‚ã‚‹å•é¡Œã¯ã€æœ€é©ãªä¿‚æ•° $c_i$ ã‚’æ±‚ã‚ã‚‹å•é¡Œã«å¸°ç€ã—ã¾ã™ã€‚

ã“ã“ã§ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯å›ºæœ‰çŠ¶æ…‹ $\psi_n$ ã¨å¯¾å¿œã™ã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼å›ºæœ‰å€¤ $E_n$ ã®ãƒšã‚¢ã‹ã‚‰ãªã‚Šã¾ã™ã€‚å¤šãã®å ´åˆã€å…ƒã® Hamiltonian $H$ ã¯æ—¢çŸ¥ã§ã‚ã‚‹ãŸã‚ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯è‡ªå‹•çš„ã«ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚çŠ¶æ…‹ $\psi_n$ ã«å¯¾å¿œã™ã‚‹ $H_{\mathrm{trial}}$ ã®å›ºæœ‰å€¤ã‚’ $E'_n$ ã€æå¤±é–¢æ•°ã‚’ $L(E'_n)$ ã¨ã™ã‚‹ã¨ã€$L$ ã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†ãª $c_i$ ã‚’æ±‚ã‚ã‚‹ã“ã¨ãŒç›®çš„ã¨ãªã‚Šã¾ã™ã€‚
$L$ ã¨ã—ã¦ã¯ã€ä¾‹ãˆã°ã€$E_n$ ã¨ $E'_n$ ã®å¹³å‡äºŒä¹—èª¤å·®

$$
L(E'_n) = \frac{1}{N}\sum_{n} (E_n - E'_n)^2
$$

ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ $c_i$ ã®æ›´æ–°æ–¹æ³•ã¨ã—ã¦ã€ç°¡å˜ã®ãŸã‚å‹¾é…é™ä¸‹æ³•ã«ã‚ˆã‚‹æœ€é©åŒ–ã‚’ç¤ºã—ã¾ã™ã€‚$\alpha$ã‚’å­¦ç¿’çŽ‡ã¨ã™ã‚‹ã¨ã€

$$
c_i \to c_i - \alpha \frac{\partial L}{\partial c_i} = c_i - \alpha \sum_n \frac{\partial L}{\partial E'_n} \frac{\partial E'_n}{\partial c_i}
$$

ã®ã‚ˆã†ã«æ›´æ–°ã•ã‚Œã¾ã™ã€‚é€£éŽ–å¾‹ã‚’ä½¿ã£ã¦ç¤ºã—ãŸã‚ˆã†ã«ã€èª¤å·®é€†ä¼æ’­æ³•ã«ã‚ˆã£ã¦ $\frac{\partial L}{\partial c_i}$ ã‚’è¨ˆç®—ã—ã€$c_i$ ã‚’æ›´æ–°ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

Ising æ¨¡åž‹ã§ã¯ã€

- å…¥åŠ›ã¯ã‚¹ãƒ”ãƒ³é…ä½ $\{\sigma_n\}$ã€å‡ºåŠ›ã¯å¯¾å¿œã™ã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼ $E_n$
- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ $\{(x_i, y_i)\}$ ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ã«ç”Ÿæˆã—ãŸ $x_i = \{\sigma_n\}$ ã‹ã‚‰ $y_i = \displaystyle{-J \sum_{\langle i, j \rangle} \sigma_i \sigma_j}$ ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã§ä½œæˆ
- è©¦è¡Œ Hamiltonian ã‚’ $\displaystyle{H_{\mathrm{trial}} = - \sum_i h_i \sigma_i}$ã¨ã—ã¦ã€ä¸Šã®è¨ˆç®—ã‚¹ã‚­ãƒ¼ãƒ ã«å¾“ã£ã¦ $h_i$ ã‚’æ›´æ–°

ã¨ãªã‚Šã¾ã™ã€‚

ã¾ãŸã€Fujita et al. (2017) ã§ã¯ã€å¼·ç›¸é–¢é›»å­ç³»ã‚’è¨˜è¿°ã™ã‚‹ Hubbard æ¨¡åž‹ã«å¯¾ã—ã¦é©ç”¨ã—ã€ä½Žã‚¨ãƒãƒ«ã‚®ãƒ¼ã«ãŠã‘ã‚‹æœ‰åŠ¹æ¨¡åž‹ã¨ã—ã¦ã‚¹ãƒ”ãƒ³æ¨¡åž‹ã‚’å­¦ç¿’ã•ã‚Œã¦ã„ã¾ã™ã€‚

## å®Ÿè£…

ä»Šå›žã¯ã€Liu et al. (2016) ã¨åŒã˜æ¨¡åž‹ã«å¯¾ã—ã¦é©ç”¨ã—ã¦ã¿ã¾ã™ã€‚
2 é …ç›®ã¯åŒä¸€ã‚»ãƒ«å†…ã® 4 ä½“ã‚¹ãƒ”ãƒ³ç›¸äº’ä½œç”¨é …ã§ã™ã€‚

$$
H = -J \sum_{\langle i, j \rangle} \sigma_i \sigma_j - K \sum_{ijkl\in \square} \sigma_i \sigma_j \sigma_k \sigma_l
$$

è©¦è¡Œ Hamiltonian ã«ã¯ã€2 ä½“ã‚¹ãƒ”ãƒ³ç›¸äº’ä½œç”¨é …ã®ã¿ã‚’å«ã‚€ Ising æ¨¡åž‹ã‚’ä»®å®šã—ã¾ã™ã€‚
ä»Šå›žã¯ã€3 æ¬¡è¿‘æŽ¥ç›¸äº’ä½œç”¨é …ã¾ã§å–ã‚Šå…¥ã‚Œã¦ã¿ã¾ã™ã€‚$\langle i, j \rangle_n$ ã¯ã€$n$ æ¬¡è¿‘æŽ¥ã®ã‚¹ãƒ”ãƒ³ãƒšã‚¢ã§ã™ã€‚

$$
H_{\mathrm{trial}} = E_0 - \tilde{J}_1\sum_{\langle i, j \rangle_1} \sigma_i \sigma_j - \tilde{J}_2\sum_{\langle i, j \rangle_2} \sigma_i\sigma_j - \tilde{J}_3\sum_{\langle i, j \rangle_3}\sigma_i\sigma_j
$$

\
Pytorch ã‚’ä½¿ã£ã¦å®Ÿè£…ã—ã¾ã™ã€‚è¤‡æ•°ã®ç³»ã§ã‚³ãƒ¼ãƒ‰ã‚’å†åˆ©ç”¨ã™ã‚‹ãŸã‚ã€ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹ã‚¯ãƒ©ã‚¹ `BaseTrainer` ã‚’ä½œæˆã—ã¾ã™ã€‚
æ¨¡åž‹ã®è©³ç´°ã«ä¾å­˜ã™ã‚‹é–¢æ•°ã¯ã‚¯ãƒ©ã‚¹ç¶™æ‰¿å¾Œä½œæˆã™ã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

```python
import os
from typing import Callable, List, Tuple, Union

import torch
from torch.nn import MSELoss
from torch.optim import Optimizer
from torch.utils.data import DataLoader, TensorDataset
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

class BaseTrainer:
    def __init__(self) -> None:
        """set basic constants of the system"""
        pass

    def effective_model(self, X: torch.Tensor, params: torch.Tensor) -> torch.Tensor:
        """calculate effective hamiltonian

        Args:
            X (torch.Tensor): input data.
            params (torch.Tensor): parameters.

        Returns:
            torch.Tensor: effective hamiltonian. (batch size)
        """
        raise NotImplementedError("This method should be overridden by derived class.")

    def original_model(self, X: torch.Tensor) -> torch.Tensor:
        """calculate original hamiltonian

        Args:
            X (torch.Tensor): input data.

        Returns:
            torch.Tensor: original hamiltonian. (batch size)
        """
        raise NotImplementedError("This method should be overridden by derived class.")

    def sample_input(self, n_samples: int) -> torch.Tensor:
        """sample input data

        Args:
            n_samples (int): number of samples.

        Returns:
            torch.Tensor: input data.
        """
        raise NotImplementedError("This method should be overridden by derived class.")

    def init_params(self) -> torch.Tensor:
        """initialize parameters

        Returns:
            torch.Tensor: parameters.
        """
        raise NotImplementedError("This method should be overridden by derived class.")
```

æ¬¡ã«ã€`BaseTrainer` ã§æ¨¡åž‹ã«ä¾ã‚‰ãªã„éƒ¨åˆ†ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
ã¾ãšã€`dataloader` ã‚’ä½œæˆã—ã¾ã™ã€‚ãƒãƒƒãƒå‡¦ç†ã‚’æ¥½ã«ã—ãŸã„ã ã‘ãªã®ã§ã€æœ¬è³ªã§ã¯ãªã„ã§ã™ã€‚

```python
    def create_dataloader(self, n_samples: int, batch_size: int) -> DataLoader:
        """create dataloader given samples and batch size

        Args:
            n_samples (int): number of data to sample.
            batch_size (int): batch size

        Returns:
            DataLoader: dataloader for training or evaluation
        """
        X = self.sample_input(n_samples)
        Y = torch.FloatTensor(self.original_model(X))
        ds = TensorDataset(X, Y)
        dataloader = DataLoader(ds, batch_size=batch_size)
        return dataloader
```

æ¬¡ã«ã€å­¦ç¿’ã‚„è©•ä¾¡ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°ã§ã™ã€‚
`loop` ã§ã¯ `dataloader` ã‹ã‚‰å…¥å‡ºåŠ›ãƒšã‚¢ã‚’å–ã‚Šå‡ºã—ã€æœ‰åŠ¹ Hamiltonian ã‚’è¨ˆç®—ã—ã¦æå¤±ã‚’è¨ˆç®—ã—ã€èª¤å·®é€†ä¼æ’­ã§å‹¾é…ã‚’è¨ˆç®—ã—ãŸå¾Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã¦ã„ã¾ã™ã€‚[^2]
[^2]: é…ã‚Œã°ã›ãªãŒã‚‰åŸ·ç­†æ™‚ã« Github Copilot ã‚’å–ã‚Šå…¥ã‚Œã¦ã¿ã¾ã—ãŸã€‚ã“ã®è‡ªç„¶æ–‡ã®ã»ã¨ã‚“ã©ã¯ Copilot ã«ã‚ˆã‚‹ææ¡ˆã‚’æŽ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ›¸ãã¨ãã«ã‚‚ä¾¿åˆ©ãã†ã§ã™ã­ã€‚

```python
    def loop(
        self,
        params: torch.Tensor,
        dataloader: DataLoader,
        loss_fn: Callable,
        optimizer: Optimizer,
        normalize_const: float = 1.0,
        mode: str = "train",
    ) -> Union[float, Tuple[float, List[float], List[float]]]:
        """loop for training, evaluation and testing

        Args:
            params (torch.Tensor): params
            dataloader (DataLoader): dataloader
            loss_fn (Callable): loss function
            optimizer (Optimizer): optimizer
            normalize_const (float, optional): normalize constant for loss. Defaults to 1.0.
            mode (str, optional): "train" or "eval". Defaults to "train".

        Returns:
            Union[float, Tuple[float, List[float], List[float]]]
            loss when mode is "train" or "eval"
            loss, original and effective hamiltonian when mode is "test"
        """

        loss_sum = 0.0
        E_original = []
        E_eff = []
        for X, Y in tqdm(dataloader, leave=False):
            Y_eff = self.effective_model(X, params)
            loss = loss_fn(Y / normalize_const, Y_eff / normalize_const)
            if mode == "train":
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
            elif mode == "test":
                E_eff += Y_eff.detach().tolist()
                E_original += Y.detach().tolist()
            loss_sum += loss.detach().item()
        loss = loss_sum / len(dataloader)
        if mode == "train" or mode == "eval":
            return loss
        elif mode == "test":
            return loss, E_original, E_eff
```

ä»¥ä¸Šã‚’ã¾ã¨ã‚ã¦ã€`__call__` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
å½“ç„¶ã®ã“ã¨ã§ã™ãŒã€å­¦ç¿’ãƒ»è©•ä¾¡ãƒ»ãƒ†ã‚¹ãƒˆã§ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
ã¾ãŸã€æå¤±é–¢æ•°ã«ã¯å¹³å‡äºŒä¹—èª¤å·® (MSE) ã‚’ä½¿ç”¨ã—ã€ `optimizer` ã¯ [`SGD`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) ã¨ [`Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam) ã‚’é¸æŠžã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚
å­¦ç¿’ã®ãƒ­ã‚°ã¯ `tensorboard` ã§é–²è¦§ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚

```python
    def __call__(
        self,
        output_dir: str,
        lr: float = 0.001,
        train_samples: int = 100,
        eval_samples: int = 100,
        test_samples: int = 100,
        epochs: int = 3,
        batch_size: int = 1,
        optimizer_name: str = "Adam",
        normalize_const: float = 1.0,
        save_model=False,
    ) -> Tuple[List[float], List[float]]:
        """run optimization

        Args:
            output_dir (str): output directory.
            lr (float, optional): learning rate. Defaults to 0.001.
            train_samples (int, optional): number of training samples. Defaults to 100.
            eval_samples (int, optional): number of evaluation samples. Defaults to 100.
            test_samples (int, optional): number of test samples. Defaults to 100.
            epochs (int, optional): number of training epochs. Defaults to 3.
            batch_size (int, optional): batch size. Defaults to 1.
            normalize_const (float, optional): normalize constant for loss. Defaults to 1.0.
            optimizer_name (str, optional): optimizer name. "SGD" & "Adam" can be used. Defaults to "SGD".

        Returns:
            Tuple[List[float], List[float]]: energies of original and effective hamiltonian
        """
        train_dataloader = self.create_dataloader(train_samples, batch_size)
        eval_dataloader = self.create_dataloader(eval_samples, 1)
        test_dataloader = self.create_dataloader(test_samples, 1)

        params = self.init_params()
        loss_fn = MSELoss()

        if optimizer_name == "Adam":
            optimizer = torch.optim.Adam([params], lr=lr)
        elif optimizer_name == "SGD":
            optimizer = torch.optim.SGD([params], lr=lr)
        else:
            raise NotImplementedError

        writer = SummaryWriter(log_dir=os.path.join(output_dir, "logs"))

        with tqdm(range(epochs)) as pbar_epoch:
            for epoch in range(epochs):
                pbar_epoch.set_description("[Epoch %d]" % (epoch + 1))
                loss = self.loop(
                    params,
                    train_dataloader,
                    loss_fn,
                    optimizer,
                    normalize_const,
                    mode="train",
                )
                writer.add_scalar("train loss", loss, epoch)
                tqdm.write(f"train loss at epoch{epoch+1}: {loss}")

                with torch.no_grad():
                    loss = self.loop(
                        params,
                        eval_dataloader,
                        loss_fn,
                        optimizer,
                        normalize_const,
                        mode="eval",
                    )
                    writer.add_scalar("eval loss", loss, epoch)
                    tqdm.write(f"eval loss at epoch{epoch+1}: {loss}")

        with torch.no_grad():
            loss, E_original, E_eff = self.loop(
                params,
                test_dataloader,
                loss_fn,
                optimizer,
                normalize_const,
                mode="test",
            )
            tqdm.write(f"test loss: {loss}")

        if save_model:
            torch.save(params, os.path.join(output_dir, "model.pth"))

        return E_original, E_eff
```

`BaseTrainer` ã‚’ç¶™æ‰¿ã—ã¦ã€ä»Šå›žé©ç”¨ã™ã‚‹ç³»ã®ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
ä¸Šã§ç¤ºã—ãŸæ¨¡åž‹ã¨æœ‰åŠ¹æ¨¡åž‹åŠã³å…¥åŠ›ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹é–¢æ•°ã¨ 4 ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–ã™ã‚‹é–¢æ•°ã‚’è¿½åŠ å®Ÿè£…ã—ã¦ã„ã‚‹ã ã‘ã§ã™ã€‚ï¼ˆæ¨¡åž‹ã®å®Ÿè£…ãŒæ±šã„ã®ãŒå¿ƒæ®‹ã‚Šã§ã™ãŒ...ï¼‰

```python
class Spin4InteractionTrainer(BaseTrainer):
    def __init__(self, J, K, Lx, Ly) -> None:
        self.J = J
        self.K = K
        self.Lx = Lx
        self.Ly = Ly

    def effective_model(self, X, params):
        batch_size = X.shape[0]
        max_n = params.shape[0]
        interact = torch.zeros((batch_size, max_n - 1))
        for i in range(self.Lx):
            for j in range(self.Ly):
                interact -= torch.stack(
                    [
                        self.neighbor_interact(n, X, i, j, self.Lx, self.Ly)
                        for n in range(1, max_n)
                    ],
                    dim=1,
                )
        interact /= 2  # 2 is for double counting
        interact = torch.concat((torch.ones((batch_size, 1)), interact), dim=1)
        H = torch.einsum("bx,x->b", interact, params)
        return H

    def original_model(self, X: torch.Tensor) -> torch.Tensor:
        n_data, Lx, Ly = X.shape
        H = torch.zeros(n_data)
        for i in range(Lx):
            for j in range(Ly):
                H += (
                    -self.J * self.neighbor_interact(1, X, i, j, self.Lx, self.Ly) / 2
                    - self.K * self.interact_cell(X, i, j, self.Lx, self.Ly) / 4
                )
        return H

    def neighbor_interact(self, n, X, i, j, Lx, Ly):
        if n != 2:
            return X[:, i, j] * (
                X[:, (i + n) % Lx, j]
                + X[:, (i - n) % Lx, j]
                + X[:, i, (j + n) % Ly]
                + X[:, i, (j - n) % Ly]
            )
        elif n == 2:
            return X[:, i, j] * (
                X[:, (i + 1) % Lx, (j + 1) % Ly]
                + X[:, (i + 1) % Lx, (j - 1) % Ly]
                + X[:, (i - 1) % Lx, (j + 1) % Ly]
                + X[:, (i - 1) % Lx, (j - 1) % Ly]
            )
        else:
            NotImplementedError

    def interact_cell(self, X, i, j, Lx, Ly):
        return X[:, i, j] * (
            X[:, (i + 1) % Lx, j]
            * X[:, i, (j + 1) % Ly]
            * X[:, (i + 1) % Lx, (j + 1) % Ly]
            + X[:, (i - 1) % Lx, j]
            * X[:, i, (j + 1) % Ly]
            * X[:, (i - 1) % Lx, (j + 1) % Ly]
            + X[:, (i + 1) % Lx, j]
            * X[:, i, (j - 1) % Ly]
            * X[:, (i + 1) % Lx, (j - 1) % Ly]
            + X[:, (i - 1) % Lx, j]
            * X[:, i, (j - 1) % Ly]
            * X[:, (i - 1) % Lx, (j - 1) % Ly]
        )

    def sample_input(self, n_samples):
        X = 1 - 2 * (torch.rand(n_samples, self.Lx, self.Ly) < 0.5)
        return X.float()

    def init_params(self):
        h = torch.ones(4, dtype=torch.float32, requires_grad=True)
        return h
```

ã¾ã¨ã‚ã‚‹ã¨ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
https://github.com/misawann/self-learningMC/blob/8b5e79891c4ae012fd895d792089b678bc5893cf/src/trainer.py#L1-L205

## å®Ÿé¨“çµæžœ

30Ã—30 ã®æ ¼å­ã«å¯¾ã—ã¦é©ç”¨ã—ãŸçµæžœã‚’ç¤ºã—ã¾ã™ã€‚
å­¦ç¿’ã¯ 1000 ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒãƒƒãƒã‚µã‚¤ã‚º 1 ã§ 10 ã‚¨ãƒãƒƒã‚¯è¡Œã„ã€åŽ³å¯†ãªç³»ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ $J=1, K=0.2$ ã¨ã—ã¾ã—ãŸã€‚ã¾ãŸã€æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¯ Adam ã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚

å­¦ç¿’æ™‚ã€è©•ä¾¡æ™‚ã¨ã‚‚ã«ååˆ†èª¤å·®ãŒå°ã•ã„ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚ãªãŠã€èª¤å·®ãŒã‹ãªã‚Šå°ã•ã„ã®ã¯ã€Liu et al. (2016)ã®çµæžœã¨æ˜Žã‚‰ã‹ã«èª¤å·®ã®ã‚ªãƒ¼ãƒ€ãƒ¼ãŒé•ã£ãŸãŸã‚ã€è‘—è€…ã®è¨˜è¿°ã‹ã‚‰æŽ¨å¯Ÿã—ã¦ç³»ã®ã‚µã‚¤ã‚ºã§ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¦ã„ã‚‹ãŸã‚ã§ã™ã€‚
![train_loss](https://storage.googleapis.com/zenn-user-upload/3bfb8c458a39-20221215.jpeg)
_å­¦ç¿’æ™‚ã®æå¤±é–¢æ•°ã€‚ç³»ã®ã‚µã‚¤ã‚ºã§ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¦ã„ã‚‹ã€‚_
![eval_loss](https://storage.googleapis.com/zenn-user-upload/04b1f03e2dcd-20221215.jpeg)
_è©•ä¾¡æ™‚ã®æå¤±é–¢æ•°ã€‚ç³»ã®ã‚µã‚¤ã‚ºã§ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¦ã„ã‚‹ã€‚_

æ¬¡ã«ã€åŽ³å¯†ãªæ¨¡åž‹ã¨å­¦ç¿’ã—ãŸæœ‰åŠ¹æ¨¡åž‹ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚ã“ã®å›³ã‹ã‚‰ã‚‚ã€ååˆ†ã«å­¦ç¿’ã§ãã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚
![energy](https://storage.googleapis.com/zenn-user-upload/14e688e9ee2b-20221215.png)
_åŽ³å¯†ãªæ¨¡åž‹ã¨å­¦ç¿’ã—ãŸæœ‰åŠ¹æ¨¡åž‹ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æ¯”è¼ƒã€‚
åŽ³å¯†ãªæ¨¡åž‹ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚½ãƒ¼ãƒˆã—ã¦ã„ã‚‹ã€‚_

ã¾ãŸã€å­¦ç¿’ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚ã‚¹ãƒ”ãƒ³é–“ã®è·é›¢ãŒè¿‘ã„ã»ã©å¯„ä¸ŽãŒå¼·ã„ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚

$$
E_0 = 0.3415,\ \tilde{J}_1 = 0.9825,\ \tilde{J}_2 = -0.0129,\ \tilde{J}_3 = 0.0097
$$

# è‡ªå·±å­¦ç¿’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³• (self-learning Monte-Carlo)

æ©Ÿæ¢°å­¦ç¿’ã‚’ç”¨ã„ã¦æœ‰åŠ¹ Hamiltonian ã‚’æ§‹ç¯‰ã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã—ãŸã€‚
æ¬¡ã«ã€ãã®å¿œç”¨å…ˆã¨ã—ã¦ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã¸ã®å¿œç”¨ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ã“ã‚Œã¯ Liu et al. (2016) ã§ææ¡ˆã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã€è‡ªå·±å­¦ç¿’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³• (self-learning Monte Carlo method) ã¨å‘½åã•ã‚Œã¾ã—ãŸã€‚

## ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³

ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã§ã¯ã€ãƒžãƒ«ã‚³ãƒ•éŽç¨‹ã«å¾“ã†ã‚ˆã†ã«çŠ¶æ…‹é·ç§»ã—ã¾ã™ã€‚ãã®ãŸã‚ã®æ¡ä»¶ã¨ã—ã¦ã€è©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶ $P(A\to B) / P(B\to A) = W(B)/W(A)$ ã‚’æº€ãŸã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã“ã§ã€$P(A\to B)$ ã¯çŠ¶æ…‹ $A$ ã‹ã‚‰çŠ¶æ…‹ $B$ ã¸ã®é·ç§»ç¢ºçŽ‡ã§ã€$W$ ã¯ç¢ºçŽ‡åˆ†å¸ƒã§ã™ã€‚
çŠ¶æ…‹ã®æ›´æ–°æ–¹æ³•ã¯å¤§ããï¼’ã¤ã«åˆ†ã‘ã‚‰ã‚Œã¾ã™ã€‚

- å±€æ‰€çš„æ›´æ–° (local update)
  - ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã‚“ã ï¼‘ã‚µã‚¤ãƒˆã®å¤‰æ•°ã‚’å¤‰ãˆã‚‹ã“ã¨ã§ã€æ¬¡ã®çŠ¶æ…‹ã‚’ææ¡ˆã—ã€ãã®çŠ¶æ…‹ã«ç§»ã‚‹ã‹ã©ã†ã‹ã‚’è©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶ã«å¾“ã£ã¦æ±ºå®š
  - æ¨¡åž‹ã«ä¾å­˜ã›ãšæ±Žç”¨çš„ã«ä½¿ãˆã‚‹
  - ä¸€æ–¹ã€è‡¨ç•Œç‚¹ä»˜è¿‘ã§ã¯ç³»ã®ã‚µã‚¤ã‚ºã¨ã¨ã‚‚ã«é…ããªã‚‹ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã‚‹
- å¤§åŸŸçš„æ›´æ–° (global update)
  - å¤šæ•°ã®ã‚µã‚¤ãƒˆã®å¤‰æ•°ãŒçµ¡ã‚€ã‚ˆã†ãªæ›´æ–°ã‚’è¡Œã†
  - æ¨¡åž‹ã®è©³ç´°ã«ä¾ã‚‹ãŸã‚æ±Žç”¨æ€§ãŒä½Žã„

ãã“ã§ã€åŠ¹çŽ‡çš„ãªå¤§åŸŸçš„æ›´æ–°æ–¹æ³•ãŒçŸ¥ã‚‰ã‚Œã¦ã„ãªã„æ¨¡åž‹ã«å¯¾ã—ã¦ã€ãã‚ŒãŒçŸ¥ã‚‰ã‚Œã¦ã„ã‚‹æœ‰åŠ¹æ¨¡åž‹ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã§è¨ˆç®—åŠ¹çŽ‡ã‚’æ”¹å–„ã§ãã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚

## ææ¡ˆæ‰‹æ³•

ã¾ãšã€åŽ³å¯†ãª Hamiltonian ã«åŸºã¥ãå±€æ‰€çš„æ›´æ–°ã«ã‚ˆã£ã¦å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
æ¬¡ã«ã€åŠ¹çŽ‡çš„ãªå¤§åŸŸçš„æ›´æ–°æ–¹æ³•ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã‚‹è©¦è¡ŒãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã‚’ä»®å®šã—ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã—ã€æœ‰åŠ¹æ¨¡åž‹ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ã¨ã¯ã„ãˆã€å¤šãã®å ´åˆã€å…ƒã® Hamiltonian ãŒæ—¢çŸ¥ã§ã‚ã‚‹ã“ã¨ãŒå¤šã„ã®ã§ã€å‰ç« ã§ç¤ºã—ãŸæ–¹æ³•ã«ã‚ˆã£ã¦å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯è‡ªå‹•ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
ãã®æœ‰åŠ¹æ¨¡åž‹ã«å¯¾ã™ã‚‹å¤§åŸŸçš„æ›´æ–°æ–¹æ³•ã«å¾“ã£ã¦æ¬¡ã®é…ä½ã‚’ææ¡ˆã—ã€å…ƒã® Hamiltonian ã«åŸºã¥ãè©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶ã‹ã‚‰æŽ¡æŠžã™ã‚‹ã‹æ£„å´ã™ã‚‹ã‹ã‚’åˆ¤æ–­ã—ã¾ã™ã€‚
![self-learning-mc](https://storage.googleapis.com/zenn-user-upload/b67740b4d1b7-20221211.jpeg)
_è‡ªå·±å­¦ç¿’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‚Liu et al. (2016) ã‚ˆã‚Šå¼•ç”¨ã€‚
(i) åŽ³å¯†ãª Hamiltonian ã«åŸºã¥ã Monte Carlo æ³•ã«ã‚ˆã£ã¦å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã€‚
(ii) åŠ¹çŽ‡çš„ãªå¤§åŸŸçš„æ›´æ–°æ–¹æ³•ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã‚‹æœ‰åŠ¹æ¨¡åž‹ã‚’æ§‹ç¯‰ã€‚
(iii) æœ‰åŠ¹æ¨¡åž‹ã«å¯¾ã™ã‚‹å¤§åŸŸçš„æ›´æ–°æ–¹æ³•ã«å¾“ã£ã¦é…ä½ã‚’ææ¡ˆã€‚
(iv) å…ƒã® Hamiltonian ã«åŸºã¥ãè©³ç´°é‡£ã‚Šåˆã„æ¡ä»¶ã‹ã‚‰æŽ¡æŠžãƒ»æ£„å´ã‚’æ±ºå®šã€‚_

## å®Ÿè£…

Ising æ¨¡åž‹ã«å¯¾ã™ã‚‹å¤§åŸŸçš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã—ã¦ã€Wolff ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ä¸€æ–¹ã€æœ‰åŠ¹æ¨¡åž‹ã®å­¦ç¿’ã®ç« ã§æ‰±ã£ãŸ 4 ã‚¹ãƒ”ãƒ³ç›¸äº’ä½œç”¨ãŒå…¥ã£ãŸæ¨¡åž‹ã«å¯¾ã—ã¦ã¯ã€åŠ¹çŽ‡çš„ãªå¤§åŸŸçš„æ›´æ–°æ–¹æ³•ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã¤ã¾ã‚Šã€æœ‰åŠ¹æ¨¡åž‹ã‚’å­¦ç¿’ã—ã€Wolff ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é«˜é€ŸåŒ–ã§ãã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚

å½“åˆã¯ã“ã¡ã‚‰ã‚‚å®Ÿè£…ã™ã‚‹ã¤ã‚‚ã‚Šã§ã—ãŸãŒã€æ—¢ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é‡ãŒå¤šããªã£ã¦ã—ã¾ã£ãŸã®ã§[^3]ã€ä»Šå¾Œæ°—ãŒå‘ã„ãŸã‚‰å®Ÿè£…ã«ã¤ã„ã¦æ›¸ã„ã¦ã¿ã‚‹ã“ã¨ã«ã—ã¾ã™ã€‚
[^3]: åŸ·ç­†ã¸ã®å–ã‚ŠæŽ›ã‹ã‚ŠãŒé…ã‹ã£ãŸã®ã§ã€ã¾ã å®Ÿè£…ã§ãã¦ã„ãªã„ã¨ã„ã†ã®ãŒäº‹å®Ÿã§ã™ã€‚

## å®Ÿé¨“çµæžœ (Liu et al., 2016)

æŠ˜è§’ãªã®ã§ã€è‘—è€…ã®å®Ÿé¨“çµæžœã‚’ç¤ºã—ã¾ã™ã€‚æ¨¡åž‹ã¯æœ‰åŠ¹æ¨¡åž‹ã®å­¦ç¿’ã§ä½¿ç”¨ã—ãŸã‚‚ã®ã¨åŒã˜ã§ã™ã€‚
è‡¨ç•Œæ¸©åº¦ã«ãŠã‘ã‚‹è‡ªå·±ç›¸é–¢é–¢æ•°ã®æ™‚é–“å¤‰åŒ–ã‚’è¡¨ã—ã¦ã„ã¦ã€ã“ã®æ¸›è¡°ãŒé€Ÿã„ï¼ˆè‡ªå·±ç›¸é–¢æ™‚é–“ãŒçŸ­ã„ï¼‰ã»ã©ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ãŒé«˜é€Ÿã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
ã“ã®çµæžœã‹ã‚‰ã€è‡ªå·±å­¦ç¿’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã®å¨åŠ›ãŒçªºãˆã¾ã™ã€‚
![slmc_reported_res](https://storage.googleapis.com/zenn-user-upload/3ef2867caddf-20221215.jpeg)
_å±€æ‰€çš„æ›´æ–°ã€è‡ªå·±å­¦ç¿’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã€Wolff ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã®è‡ªå·±ç›¸é–¢é–¢æ•°ã®æ¸›è¡°ã€‚
Liu et al. (2016) ã‚ˆã‚Šå¼•ç”¨ã€‚_

# ã•ã„ã”ã«

æ©Ÿæ¢°å­¦ç¿’ Ã— ç‰©ç†ã®ä¸€ä¾‹ã¨ã—ã¦ã€æœ‰åŠ¹æ¨¡åž‹ã®æ§‹ç¯‰ã¨è‡ªå·±å­¦ç¿’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã‚’æ‰±ã£ã¦ã¿ã¾ã—ãŸã€‚
å®Ÿè£…ã‚„ç™ºæƒ³ã¯ã‚·ãƒ³ãƒ—ãƒ«ãªãŒã‚‰é©ç”¨ç¯„å›²ã®åºƒã„æ‰‹æ³•ã§ã€å®Ÿé¨“ã—ç”²æ–ãŒã‚ã‚Šãã†ã§ã™ã€‚
è‡ªå·±å­¦ç¿’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã¯ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é‡ã¨å·¥æ•°ã®éƒ½åˆä¸Šã€å®Ÿè£…ã‚’ç´¹ä»‹ã§ãã¾ã›ã‚“ã§ã—ãŸãŒã€ã„ã¤ã‹æ›¸ã„ã¦ã¿ã‚ˆã†ã¨æ€ã„ã¾ã™ã€‚
è‡ªå·±å­¦ç¿’ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã‚’é‡å­ç³»ã«é©ç”¨ã™ã‚‹å ´åˆã‚‚æ°—ãŒå‘ã„ãŸã‚‰å®Ÿè£…ã—ã¦ã¿ã¾ã™ã€‚

# å‚è€ƒæ–‡çŒ®

[1] Fujita, H., Nakagawa, Y. O., Sugiura, S., & Oshikawa, M. (2018). Construction of Hamiltonians by supervised learning of energy and entanglement spectra. Physical Review B, 97(7), 075114.
[2] Liu, J., Qi, Y., Meng, Z. Y., & Fu, L. (2017). Self-learning monte carlo method. Physical Review B, 95(4), 041101.
[3] [Naoki, K. (2019). Lecture 2: Meanfield approximation, variational principle and Landau expansion.](https://kawashima.issp.u-tokyo.ac.jp/wp/wp-content/uploads/2019/04/Lecture02-v04h.pdf) ç‰©å·¥ã®æ¼”ç¿’ã§ã¯å¤‰åˆ†è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã¨è‡ªç”±ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®å·®ãŒéžè² ã§ã‚ã‚‹ã“ã¨ã¯ç¤ºã—ã¾ã™ãŒã€ãã®å·®ãŒ KL divergence ã«ãªã‚‹ã“ã¨ã¯ã“ã¡ã‚‰ã®è³‡æ–™ã‚’å‚è€ƒã«ã—ã¾ã—ãŸã€‚

# ä½™è«‡

Hamiltonian ã‚’å­¦ç¿’ã™ã‚‹æ–‡è„ˆã§ã€Hamiltonian Neural Network (Greydanus, 2019) ã¨ã„ã†ã‚‚ã®ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€å¤å…¸åŠ›å­¦ç³»ã«ãŠã„ã¦ã€è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›ã€æ­£æº–æ–¹ç¨‹å¼ã®æ®‹å·®ã‚’æå¤±é–¢æ•°ã¨ã—ã¦ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å­¦ç¿’ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€ãã®ç³»ãŒå¾“ã†æ–¹ç¨‹å¼ã‚’æŽ¨å®šã™ã‚‹æ‰‹æ³•ã§ã™ã€‚å…ƒã€…ã¯ã“ã‚Œã‚’å®Ÿè£…ã™ã‚‹ã¤ã‚‚ã‚Šã§ã—ãŸãŒã€è‘—è€…å®Ÿè£…ãŒå…¬é–‹ã•ã‚Œã¦ã„ã‚‹ä¸Šã«ãã‚ŒãŒã‚ã¾ã‚Šã«ã‚‚ãƒªãƒƒãƒã ã£ãŸã®ã§ã‚„ã‚ã¾ã—ãŸã€‚
https://github.com/greydanus/hamiltonian-nn
\
ã‚¢ãƒ‰ã‚«ãƒ¬ã®ä»–ã®å€™è£œã¨ã—ã¦ã¯ä»¥ä¸‹ã‚’è€ƒãˆã¦ã„ã¾ã—ãŸã€‚

- ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã®å†å¸°æ€§ã«é–¢ã™ã‚‹ Polya ã®å®šç†
  - æ§˜ã€…ãªè¨¼æ˜ŽãŒã‚ã‚‹ã‚‰ã—ã„
- æ·±å±¤å­¦ç¿’ã§ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
  - æ¥­å‹™ã§ã‚„ã£ã¦ã„ã¦æ™®æ®µã‹ã‚‰ã‚¤ãƒ³ãƒ—ãƒƒãƒˆãƒ»ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆãŒã‚ã‚‹ã®ã§ã‚„ã‚ã¾ã—ãŸ
- è¶£å‘³ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã«ã¤ã„ã¦
  - è‡ªå·±æº€ã«ãªã£ã¦ã—ã¾ã†ã®ã§æ–­å¿µ

\
Zenn ã§åŸ·ç­†ã™ã‚‹ã®ã¯åˆã‚ã¦ã§ã—ãŸãŒã€Github é€£æºã¨ VSCode æ‹¡å¼µ (Zenn Editor) ãŒä¾¿åˆ©ã§ã—ãŸã€‚ä»¥ä¸‹ã®è¨˜äº‹ã‚’å‚è€ƒã«ã—ã¾ã—ãŸã€‚

- [GitHub ãƒªãƒã‚¸ãƒˆãƒªã§ Zenn ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç®¡ç†ã™ã‚‹](https://zenn.dev/zenn/articles/connect-to-github)
- [Zenn ã®åŸ·ç­†ã‚’æ”¯æ´ã™ã‚‹ VSCode æ‹¡å¼µ Zenn Editor](https://zenn.dev/negokaz/articles/aa4e12b76d516597a00e)
  ![zenn-editor](https://storage.googleapis.com/zenn-user-upload/b0cebe1499ca-20221215.jpeg)
